The group of ninety-three students receiving no college credit had a mean score of 4.17 on the
validation test with a sample standard deviation of 3.70. For the twenty-eight students who received
credit from a high school dual-enrollment class, the mean score was 4.61 with a sample standard
deviation of 4.28. Is there a significant difference in these means at the $\alpha=0.01$ level? Assume
the variances are equal.\\\\

\begin{solution}\renewcommand{\qedsymbol}{}\ \\
    We have $n=93$, $\bar{x}=4.17$, $s_x=3.7$, $m=28$, $\bar{y}=4.61$, $s_y=4.28$, and $\alpha=0.01$. We
    also assume that $\sigma_x^2=\sigma_y^2$. We will test $H_0:\mu_x=\mu_y$ against $H_1:\mu_x<\mu_y$.
    Well, we have a pooled variance of
    
    $$s_p^2=\frac{(3.7)^2(92)+(4.28)^2(27)}{119}=14.74014$$
    
    So, we have a $t$ value of
    
    $$t=\frac{4.17-4.61}{(\sqrt{14.74014(\frac{1}{93}+\frac{1}{28})})}=-0.53165$$
    
    Now, we have $93+28-2=119$ degrees of freedom, so $-t_{\alpha}=-2.33$. Therefore $t>t_{\alpha}$, so
    we do not have evidence to reject $H_0$, and thus we fail to reject $H_0$.

\end{solution}