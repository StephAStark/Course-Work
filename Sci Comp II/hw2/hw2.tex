\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{accents}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode}
\setlength{\oddsidemargin}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-.55in}
\setlength{\textheight}{9in}
\pagestyle{empty}
\renewcommand \d{\displaystyle}
\begin{document}
\noindent Stephanie Klumpe

\noindent Math 6680

\noindent HW 2

5.1. Modify Program 9 to compute and plot the maximum error over $[-1,1]$ for equispaced and Chebyshev interpolation
on a log scale as a function of N. What asymptotic divergence and convergence constants do you observe for these two
cases? (Confine your attention to small enough values of $N$ that rounding errors are not dominant.) Now, based on
the potential theory of this chapter, determine exactly what these geometric constants should be. How closely do your
numerical experiments match the theoretical answers?
\footnote{I accidentally did this problem and didn't want to delete all of the work, so I kept it in.}\\\\

Here, we are choosing $N$ to be multiples of 4 between 4 and 28.\\
\begin{figure}[htp]
\centering
\includegraphics[scale=0.2]{5_14.PNG}
\caption{$N=4$}
\end{figure}
\begin{figure}[htp]
\centering
\includegraphics[scale=0.2]{5_18.PNG}
\caption{$N=8$}
\end{figure}
\begin{figure}[htp]
\centering
\includegraphics[scale=0.2]{5_112.PNG}
\caption{$N=12$}
\end{figure}
\begin{figure}[htp]
\centering
\includegraphics[scale=0.2]{5_116.PNG}
\caption{$N=16$}
\end{figure}
\begin{figure}[htp]
\centering
\includegraphics[scale=0.2]{5_120.PNG}
\caption{$N=20$}
\end{figure}
\begin{figure}[htp]
\centering
\includegraphics[scale=0.2]{5_124.PNG}
\caption{$N=24$}
\end{figure}
\begin{figure}[htp]
\centering
\includegraphics[scale=0.2]{5_128.PNG}
\caption{$N=28$}
\end{figure}

\newpage

Now, looking at the two plots of the error vs $N$, we see that for the equidistal points, there is no convergence.
Rather, there is divergence towards infinity and this divergence happens exponentialy. Similarly, for the Chebyshev
nodes, there is an exponential plotting, however, this is exponentially convergent.\\

\begin{figure}[htp]
\centering
\includegraphics[scale=0.12]{5_1equi.PNG}
\end{figure}
\begin{figure}[htp]
\centering
\includegraphics[scale=0.12]{5_1cheb.PNG}\\
\end{figure}

\newpage
\lstinputlisting{problem5_1.m}
\newpage

\newpage

5.3. Let $E_N = \inf_p||p(x)-e^x||_{\infty}$, where $||f||_{\infty}=\sup_{x\in[-1,1]}|f (x)|$, denote the error in
degree $N$ minimax polynomial approximation to $e^x$ on $[-1, 1]$. (a) One candidate approximation $p(x)$ would be
the Taylor series truncated at degree $N$. From this approximation, derive the bound $E_N<((N + 2)/(N + 1))/(N + 1)!$
for $N\geq0$. (b) In fact, the truncated Taylor series falls short of optimal by a factor of about $2^N$, 
for it is known (see equation (6.75) of [Mei67]) that as $N\rightarrow\infty$, $E_N\sim2^{-N}/(N + 1)!$. Modify
Program 9 to produce a plot showing this asymptotic formula, the upper bound of (a), the error when $p(x)$ is obtained 
from interpolation in Chebyshev points, and the same for equispaced points, all as a function of $N$ for
$N = 1, 2, 3,\ldots,12$. Comment on the results.\\\\

We know that the Taylor series expansion of $e^x$ about $x=0$ is given by $\sum\frac{x^n}{n!}$. So, the truncated
series is $1+x+\frac{x^2}{2}+\ldots+\frac{x^N}{N!}$. Hence, $|p(x)-e^x|=|R_N(x)|$ where $R_N(x)$ is the remainder
of the exact Taylor expansion of $e^x$. We also know that such an expansion exists as $e^x$ is an analytic function.
So,

$$E_N=\sup_{x\in[-1,1]}|\sum_{k=N+1}^{\infty}\frac{x^k}{k!}|$$

$$=\sum_{k=1}^{\infty}\frac{1}{(N+k)!}$$

$$=\frac{1}{(N+1)!}(1+\frac{1}{N+2}+\frac{1}{(N+2)(N+3)}+\dots)$$

$$<\frac{1}{(N+1)!}(\sum_{k=0}^{\infty}\frac{1}{(N+2)^k})$$

Here, we have a geometric series, so we get the equality

$$=\frac{1}{(N+1)!}\frac{1}{1-\frac{1}{N+2}}$$

$$=\frac{1}{(N+1)!}\frac{1}{\frac{N+2}{N+2}-\frac{1}{N-2}}$$

$$=\frac{N+2}{N+1}\frac{1}{(N+1)!}$$

as desired.\\

Now, plotting the error bound we found, the known error bound, and the error of the approximations of $e^x$ yields us
graph:

\begin{figure}[htp]
\centering
\includegraphics[scale=0.12]{5_3_1.PNG}
\end{figure}

\lstinputlisting{problem5_3.m}
\newpage

6.1. If $x_0,x_1,\ldots,x_N\in\mathbb{R}$ are distinct, then the cardinal function $p_j(x)$ defined by

$$p_j(x)=\frac{1}{a_j}\prod_{k=0,\neq j}^N(x-x_k),\;\;\;a_j=\prod_{k=0,\neq j}^N(x_j-x_k)$$

is the unique polynomial interpolant of degree $N$ to the values 1 at $x_j$ and 0 at $x_k$, $k\neq j$.
Take the logarithm and differentiate to obtain

$$p_j'(x)=p_j(x)\sum_{k=0,\neq j}^N(x-x_k)^{-1},$$

and derive the formulas

$$D_{ij}=\frac{1}{a_j}\prod_{k=0,\neq j}^N(x_i-x_k)=\frac{a_i}{a_j(x_i-x_j)}\;\;\;(i\neq j)$$

and

$$D_{jj}=\sum_{k=0,\neq j}^N(x_j-x_k)^{-1}$$\\\\

Letting $p_j(x)$ and $a_j$ be as given above, we take the natural logarithm of both sides. This gives us

$$\ln(p_j(x))=\ln(\prod_{k=0,\neq j}^N\frac{(x-x_k)}{(x_j-x_k)})=\sum_{k=0,\neq j}^N\ln(\frac{x-x_k}{x_j-x_k})$$

$$=\sum_{k=0,\neq j}^N\ln(x-x_k)-\sum_{k=0,\neq j}^N\ln(x_j-x_k)$$

Now differentiating both sides yields

$$\frac{p'_j(x)}{p_j(x)}=\sum_{k=0,\neq j}^N\frac{1}{(x-x_k)}-0$$

which gives us the desired equation

$$p_j'(x)=p_j(x)\sum_{k=0,\neq j}^N(x-x_k)^{-1}$$

\newpage

6.8. Let $D_N$ be the usual Chebyshev differentiation matrix. Show that the power $(D_N)^{N+1}$ is identically
equal to zero. Now try it on the computer for $N=5$ and $20$ and report the computed 2-norms $||(D_5)^6||_2$
and $||(D_{20})^{21}||_2$. Discuss.\\\\

We know that the Chebyshev differentiation matrix, $D_N$, is a result of differentiating the $N^{th}$ degree Chebyshev 
polynomial. By the nature of the polynomials, we have that they are given by $\sum_{i=1}^Nc_ix^i$ where $c_i$ are the 
coefficients found by interpolating through the Chebyshev nodes. So, taking the derivative would give us an $(N-1)th$ 
degree polynomial. To find the second order differentiation matrix, we apply $D_N$ to $D_N$ to get $D_N^2$. That is, we 
take the second derivative of the Chebyshev polynomial. Continuing the process, we see that

$$(\sum_{i=1}^Nc_ix^i)^{(N)}=c_N$$

Hence,

$$(\sum_{i=1}^Nc_ix^i)^{(N+1)}=(c_N)'=0$$

So, applying $D_N$ to $(D_N)^N$ gives us $(D_N)^{N+1}=0$ as desired.\\

Now, with the following code, we see that we do get a number very close to zero relative to Matlab's precision for
$N=5$, however, we get an astronomically large number for $N=20$. This is due to the roundinjg and floating number
capapbilities for the program. So while theoretically, the matrix to the 21st power would yield 0, that doesn't quite
work in this particular instance of numerical computing.

\newpage
\lstinputlisting{problem6_8.m}
\newpage

7.1. Modify Program 13 so that instead of polyval and polyfit, it uses the more stable formula of barycentric
interpolation

$$p(x)=\sum_{j=0}^N\frac{a_j^{-1}u_j}{x-x_j}/\sum_{j=0}^N\frac{a_j^{-1}}{x-x_j}$$

where $\{a_j\}$ are defined by (6.7). Experiment with various interpolation problems (such as that of Exercise 5.1)
and find evidence of the enhanced stability of this method.\\\\

Using the barycentric interpolation, we have the following code. We test it with three different conditions for our 
ODE and get these three graphs. We can see that this interpolation method works well and is quite robust.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.2]{7_1exp.PNG}
\caption{$e^{4x}$}
\end{figure}
\begin{figure}[htp]
\centering
\includegraphics[scale=0.2]{7_1sin.PNG}
\caption{$\sin(x)$}
\end{figure}
\begin{figure}[htp]
\centering
\includegraphics[scale=0.2]{7_1exp2.PNG}
\caption{$e^{x^2}$}
\end{figure}

\newpage
\lstinputlisting{problem7_1.m}
\newpage

7.2. Solve the boundary value problem $u_{xx}+4u_x+e^xu=\sin(8x)$ numerically on $[-1,1]$ with boundary conditions
$u(\pm1)=0$. To ten digits of accuracy, what is $u(0)$?\\\\

For this problem, we have both a first order and a second order term in our equation, so we will use a
first and second order Chebyshev matrix. After running through the code below, we have that
$u(0)\approx0.009597857230$ and we also get the following graph with $N=50$.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.14]{7_2.PNG}
\end{figure}

\newpage
\lstinputlisting{problem7_2.m}
\newpage

8.3. Modify Program 12 (p. 57) to make use of chebfft instead of cheb. The results should be the same as in Output 12,
except for rounding errors. Are the effects of rounding errors smaller or larger than before?\\\\

Below are the graph outputs of program 12, the first of which is using cheb and the second using chebfft:\\

\begin{figure}[htp]
\centering
\includegraphics[scale=0.14]{8_3cheb.PNG}
\caption{cheb}
\end{figure}
\begin{figure}[htp]
\centering
\includegraphics[scale=0.14]{8_3chebfft.PNG}
\caption{chebfft}
\end{figure}

We see that the round off error is the only difference between the two graphs. It can also be seen that the rounding
error for the chebfft function is slightly less than that of the regular cheb function.

\newpage
\lstinputlisting{problem8_3.m}
\newpage

8.5. Find a way to modify your program of Exercise 8.4, as in Exercise 3.8 but now for a second-order problem, to make
use of matrix exponentials rather than time discretization. What effect does this have on the computation time?\\\\

Using the code that we worked through in class, we have the following code. From the code, we have that the elapsed time
is about $0.0065$ seconds. For the original code, adding in the tic/toc function, we get that the average elapsed time is
about $0.5$ seconds.

\newpage
\lstinputlisting{problem8_5.m}
\newpage

8.6. Write a code chebfft2 for second-order differentiation by the FFT, and show by examples that it matches the
results obtained by matrices, apart from rounding errors.
\footnote{Again, this was not an assigned problem, however I did quite a bit of work for it and didn't want to delete it.}\\\\

Here, we used the functions $\sin(x), e^x,$ and $e^{-x^2}$ as examples to demonstrate the chebfft2 function.
In order to write this function, all we needed to do is repeat the FFT, inverse FFT, and deriving the algebraic
polynomial steps just on the derivative vector. So, we get the following graphs.\\

\begin{figure}[htp]
\centering
\includegraphics[scale=0.10]{8_5sin.PNG}
\caption{$\sin(x)$}
\end{figure}
\begin{figure}[htp]
\centering
\includegraphics[scale=0.10]{8_5exp.PNG}
\caption{$e^x$}
\end{figure}
\begin{figure}[htp]
\centering
\includegraphics[scale=0.10]{8_5expx2.PNG}
\caption{$e^{-x^2}$}
\end{figure}


\newpage
\lstinputlisting{problem8_6.m}






\end{document}